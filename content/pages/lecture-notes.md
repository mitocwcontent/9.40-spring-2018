---
content_type: page
title: Lecture Notes
uid: 8f6c4243-744a-9b41-6623-a021d0178db2
---

| Lec # | Learning Objectives | Lecture Notes |
| --- | --- | --- |
| 1 |  {{< br >}}{{< br >}} *   To understand how the timescale of diffusion relates to length scales{{< br >}}*   To understand how concentration gradients lead to currents (Fick’s First Law){{< br >}}*   To understand how charge drift in an electric field leads to currents (Ohm’s Law and resistivity) {{< br >}}{{< br >}}  | [Overview and Ionic Currents (PDF - 1.7MB)]({{< baseurl >}}/resources/mit9_40s18_lec01) |
| 2 |  {{< br >}}{{< br >}} *   To understand how neurons respond to injected currents{{< br >}}*   To understand how membrane capacitance and resistance allows neurons to integrate or smooth their inputs over time (RC model){{< br >}}*   To understand how to derive the differential equations for the RC model{{< br >}}*   To be able to sketch the response of an RC neuron to different current inputs{{< br >}}*   To understand where the ‘batteries’ of a neuron come from {{< br >}}{{< br >}}  | [RC Circuit and Nernst Potential (PDF - 2.7MB)]({{< baseurl >}}/resources/mit9_40s18_lec02) |
| 3 |  {{< br >}}{{< br >}} *   To be able to construct a simplified model neuron by replacing the complex spike generating mechanisms of the real neuron (HH model) with a simplified spike generating mechanism{{< br >}}*   To understand the processes that neurons spend most of their time doing which is integrating inputs in the interval between spikes{{< br >}}*   To be able to create a quantitative description of the firing rate of neurons in response to current inputs{{< br >}}*   To provide an easy-to implement model that captures the basic properties of spiking neurons {{< br >}}{{< br >}}  | ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Nernst Potential and Integrate and Fire Models​ (PDF - 4.1MB)]({{< baseurl >}}/resources/mit9_40s18_lec03) |
| 4 |  {{< br >}}{{< br >}} *   To be able to draw the circuit diagram of the HH model{{< br >}}*   Understand what a voltage clamp is and how it works{{< br >}}*   Be able to plot the voltage and time dependence of the potassium current and conductance{{< br >}}*   Be able to explain the time and voltage dependence of the potassium conductance in terms of Hodgkin-Huxley gating variables {{< br >}}{{< br >}}  | [Hodgkin Huxley Model Part 1 (PDF - 6.3MB)]({{< baseurl >}}/resources/mit9_40s18_lec04) |
| 5 | ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Hodgkin Huxley Model Part 2 (PDF - 3.3MB)]({{< baseurl >}}/resources/mit9_40s18_lec05) |
| 6 |  {{< br >}}{{< br >}} *   To be able to draw the ‘circuit diagram’ of a dendrite{{< br >}}*   Be able to plot the voltage in a dendrite as a function of distance for leaky and non-leaky dendrite, and understand the concept of a length constant{{< br >}}*   Know how length constant depends on dendritic radius{{< br >}}*   Understand the concept of electrotonic length{{< br >}}*   Be able to draw the circuit diagram a two-compartment model {{< br >}}{{< br >}}  | ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Dendrites (PDF - 3.2MB)]({{< baseurl >}}/resources/mit9_40s18_lec06) |
| 7 |  {{< br >}}{{< br >}} *   Be able to add a synapse in an equivalent circuit model{{< br >}}*   To describe a simple model of synaptic transmission{{< br >}}*   To be able to describe synaptic transmission as a convolution of a linear kernel with a spike train{{< br >}}*   To understand synaptic saturation{{< br >}}*   To understand the different functions of somatic and dendritic inhibition {{< br >}}{{< br >}}  | ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Synapses (PDF - 3.1MB)]({{< baseurl >}}/resources/mit9_40s18_lec07) |
| 8 |  {{< br >}}{{< br >}} *   To understand the origin of extracellular spike waveforms and local field potentials{{< br >}}*   To understand how to extract local field potentials and spike signals by low-pass and high-pass filtering, respectively{{< br >}}*   To be able to extract spike times as a threshold crossing{{< br >}}*   To understand what a peri-stimulus time histogram (PSTH) and a tuning curve is{{< br >}}*   To know how to compute the firing rate of a neuron by smoothing a spike train {{< br >}}{{< br >}}  | [Spike Trains (PDF - 2.6MB)]({{< baseurl >}}/resources/mit9_40s18_lec08) |
| 9 |  {{< br >}}{{< br >}} *   To be able to mathematically describe a neural response as a linear filter followed by a nonlinear function.{{< br >}}    *   A correlation of a spatial receptive field with the stimulus{{< br >}}    *   A convolution of a temporal receptive field with the stimulus{{< br >}}*   To understand the concept of a Spatio-temporal Receptive Field (STRF) and the concept of ‘separability’{{< br >}}*   To understand the idea of a Spike Triggered Average and how to use it to compute a Spatio-temporal Receptive Field and a Spectro-temporal Receptive Field (STRF). {{< br >}}{{< br >}}  | [Receptive Fields (PDF - 2.1MB)]({{< baseurl >}}/resources/mit9_40s18_lec09) |
| 10 |  {{< br >}}{{< br >}} *   Spike trains are probabilistic (Poisson Process){{< br >}}*   Be able to use measures of spike train variability{{< br >}}    *   Fano Factor{{< br >}}    *   Interspike Interval (ISI){{< br >}}*   Understand convolution, cross-correlation, and autocorrelation functions{{< br >}}*   Understand the concept of a Fourier series {{< br >}}{{< br >}}  | [Time Series (PDF - 4.5MB)]({{< baseurl >}}/resources/mit9_40s18_lec10) |
| 11 |  {{< br >}}{{< br >}} *   Fourier series for symmetric and asymmetric functions{{< br >}}*   Complex Fourier series{{< br >}}*   Fourier transform{{< br >}}*   Discrete Fourier transform (Fast Fourier Transform - FFT){{< br >}}*   Power spectrum {{< br >}}{{< br >}}  | [Spectral Analysis Part 1 (PDF - 4.3MB)]({{< baseurl >}}/resources/mit9_40s18_lec11) |
| 12 |  {{< br >}}{{< br >}} *   Fourier Transform Pairs{{< br >}}*   Convolution Theorem{{< br >}}*   Gaussian Noise (Fourier Transform and Power Spectrum){{< br >}}*   Spectral Estimation{{< br >}}    *   Filtering in the frequency domain{{< br >}}    *   Wiener-Kinchine Theorem{{< br >}}*   Shannon-Nyquist Theorem (and zero padding){{< br >}}*   Line noise removal {{< br >}}{{< br >}}  | [Spectral Analysis Part 2 (PDF - 3.1MB)]({{< baseurl >}}/resources/mit9_40s18_lec12) |
| 13 |  {{< br >}}{{< br >}} *   Brief review of Fourier transform pairs and convolution theorem{{< br >}}*   Spectral estimation{{< br >}}    *   Windows and Tapers{{< br >}}*   Spectrograms{{< br >}}*   Multi-taper spectral analysis{{< br >}}    *   How to design the best tapers (DPSS){{< br >}}    *   Controlling the time-bandwith product{{< br >}}*   Advanced filtering methods {{< br >}}{{< br >}}  | [Spectral Analysis Part 3 (PDF - 2.2MB)]({{< baseurl >}}/resources/mit9_40s18_lec13) |
| 14 |  {{< br >}}{{< br >}} *   Derive a mathematically tractable model of neural networks (the rate model){{< br >}}*   Building receptive fields with neural networks{{< br >}}*   Vector notation and vector algebra{{< br >}}*   Neural networks for classification{{< br >}}*   Perceptrons {{< br >}}{{< br >}}  | [Rate Models and Perceptrons (PDF - 3.9MB)]({{< baseurl >}}/resources/mit9_40s18_lec14) |
| 15 |  {{< br >}}{{< br >}} *   Perceptrons and perceptron learning rule{{< br >}}*   Neuronal logic, linear separability, and invariance{{< br >}}*   Two-layer feedforward networks{{< br >}}*   Matrix algebra review{{< br >}}*   Matrix transformations {{< br >}}{{< br >}}  | ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Matrix Operations (PDF - 4.0MB)]({{< baseurl >}}/resources/mit9_40s18_lec15) |
| 16 |  {{< br >}}{{< br >}} *   More on two-layer feed-forward networks{{< br >}}*   Matrix transformations (rotated transformations){{< br >}}*   Basis sets{{< br >}}*   Linear independence{{< br >}}*   Change of basis {{< br >}}{{< br >}}  | [Basis Sets (PDF - 2.8MB)]({{< baseurl >}}/resources/mit9_40s18_lec16) |
| 17 |  {{< br >}}{{< br >}} *   Eigenvectors and eigenvalues{{< br >}}*   Variance and multivariate Gaussian distributions{{< br >}}*   Computing a covariance matrix from data{{< br >}}*   Principal Components Analysis (PCA) {{< br >}}{{< br >}}  | [Principal Components Analysis​ (PDF - 4.8MB)]({{< baseurl >}}/resources/mit9_40s18_lec17) |
| 18 |  {{< br >}}{{< br >}} *   Mathematical description of recurrent networks{{< br >}}*   Dynamics in simple autapse networks{{< br >}}*   Dynamics in fully recurrent networks{{< br >}}*   Recurrent networks for storing memories{{< br >}}*   Recurrent networks for decision making (winner-take-all) {{< br >}}{{< br >}}  | ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Recurrent Networks (PDF - 2.2MB)]({{< baseurl >}}/resources/mit9_40s18_lec18) |
| 19 |  {{< br >}}{{< br >}} *   Recurrent neural networks and memory{{< br >}}*   The oculomotor system as a model of short term memory and neural integration{{< br >}}*   Stability in neural integrators{{< br >}}*   Learning in neural integrators {{< br >}}{{< br >}}  | [Neural Integrators (PDF - 2.0MB)]({{< baseurl >}}/resources/mit9_40s18_lec19) |
| 20 |  {{< br >}}{{< br >}} *   Recurrent networks with lambda greater than one{{< br >}}    *   Attractors{{< br >}}*   Winner-take-all networks{{< br >}}*   Attractor networks for long-term memory (Hopfield model){{< br >}}*   Energy landscape{{< br >}}*   Hopfield network capacity {{< br >}}{{< br >}}  | ![This resource may not render correctly in a screen reader.](/images/inacessible.gif)[Hopfield Networks (PDF - 2.7MB)]({{< baseurl >}}/resources/mit9_40s18_lec20)